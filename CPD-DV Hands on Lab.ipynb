{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Cloud Pak for Data - Multi-Cloud Virtualization Hands-on Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to the IBM Cloud Pak for Data Multi-Cloud Virtualization Hands on Lab. \n",
    "\n",
    "In this lab you analyze data from multiple data sources, from across multiple Clouds, without copying data into a warehouse.\n",
    "\n",
    "This hands-on lab uses live databases, were data is “virtually” available through the IBM Cloud Pak for Data Virtualization Service. This makes it easy to analyze data from across your multi-cloud enterprise using tools like, Jupyter Notebooks, Watson Studio or your favorite reporting tool like Cognos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to find this sample online\n",
    "You can find a copy of this notebook on GITHUB at https://github.com/Db2-DTE-POC/CPDDVLAB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to add this notebook to a IBM Cloud Pak for Data Project\n",
    "1. Click the three bar main navigation menu\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/2.42.03 Three Bar.png\">\n",
    "    \n",
    "2. Select **Projects**    \n",
    "    \n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.17.07 Projects.png\">\n",
    "    \n",
    "2. Either select the **existing project** that is the same as your LABDATAENGINEER id.\n",
    "3. From the My Projects screen click **Add to project** at the top right of the screen\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.17.48 Add to project.png\">\n",
    "    \n",
    "4. Click **Notebook**\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.17.59 Notebook.png\">\n",
    "\n",
    "5. Click **From URL**\n",
    "6. Enter **Data Virtualization Hands on Lab** in the project **Name** field\n",
    "7. Copy and paste the following link into the **Notebook URL** field:\n",
    "    https://github.com/Db2-DTE-POC/CPDDVLAB/blob/master/CPD-DV%20Hands%20on%20Lab.ipynb\n",
    "8. Add an optional description\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.25.23 New notebook.png\">\n",
    "\n",
    "9. Click **Create Notebook**\n",
    "10. Click the **pencil icon** at the top of your new notebook to actively run your new notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The business problem and the landscape\n",
    "The Acme Company needs timely analysis of stock trading data from multiple source systems. \n",
    "\n",
    "Their data science and development teams needs access to:\n",
    "* Customer data\n",
    "* Account data\n",
    "* Trading data\n",
    "* Stock history and Symbol data\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/CPDDVLandscape.png\">\n",
    "\n",
    "The data sources are running on premises and on the cloud. In this example many of the databases are also running on OpenShift but they could be managed, virtual or bare-metal cloud installations. IBM Cloud Pak for Data doesn't care. Enterprise DB (Postgres) is also running in the Cloud.Mongo andn Informix are running on premises. Finally we also have a VSAM file on zOS leveraging the Data Virtualization Manager for zOS. \n",
    "\n",
    "To simply access for Data Scientists and Developers the Acme team want to make all their data look like it is coming from a single database. They also want to combine data to create simple to use tables.\n",
    "\n",
    "In the past, Acme built a dedicated data warehouse, and then created ETL (Export, Transform and Load) job to move data from each data source into the warehouse were it could be combined. Now you can just virtualize your data without moving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this lab you learn how to:\n",
    "\n",
    "* Sign into IBM Cloud Pak for Data using your own Data Engineer and Data Scientist (User) userids\n",
    "* Connect to different data sources, on premesis and across a multi-vendor Cloud\n",
    "* Make remote data from across your multi-vendor enterprise look and act like local tables in a single database\n",
    "* Make combining complex data and queries simple even for basic users\n",
    "* Capture complex SQL in easy to consume VIEWs that act just like simple tables\n",
    "* Ensure that users can securely access even complex data across multiple sources \n",
    "* Use roles and privledges to ensure that only the right user may see the right data\n",
    "* Make development easy by connecting to your virtualized data using Analytic tools and Appliction from outside of IBM Cloud Pak for Data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Jupyter notebooks\n",
    "You are now officially using a Jupyter notebook! If this is your first time using a Jupyter notebook you might want to go through the [An Introduction to Jupyter Notebooks](http://localhost:8888/notebooks/An_Introduction_to_Jupyter_Notebooks.ipynb). The introduction shows you some of the basics of using a notebook, including how to create the cells, run code, and save files for future use. \n",
    "\n",
    "Jupyter notebooks are based on IPython which started in development in the 2006/7 timeframe. The existing Python interpreter was limited in functionality and work was started to create a richer development environment. By 2011 the development efforts resulted in IPython being released (http://blog.fperez.org/2012/01/ipython-notebook-historical.html).\n",
    "\n",
    "Jupyter notebooks were a spinoff (2014) from the original IPython project. IPython continues to be the kernel that Jupyter runs on, but the notebooks are now a project on their own.\n",
    "\n",
    "Jupyter notebooks run in a browser and communicate to the backend IPython server which renders this content. These notebooks are used extensively by data scientists and anyone wanting to document, plot, and execute their code in an interactive environment. The beauty of Jupyter notebooks is that you document what you do as you go along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to IBM Cloud Pak for Data\n",
    "For this lab you will be assigned two IBM Cloud Pak for Data User IDs: A Data Engineer userid and and end-user userid. Check with the lab coordinator which userid and passwords you should use.\n",
    "* **Engineer:**\n",
    "    * ID: LABDATAENGINEERx\n",
    "    * PASSWORD: xxx\n",
    "* **User:**\n",
    "    * ID: LABUSERx\n",
    "    * PASSWORD: xxx\n",
    "\n",
    "To get started, sign in using you Engineer id:\n",
    "1. Right-click the following link and select **open link in new window** to open the IBM Cloud Pak for Data Console: https://services-uscentral.skytap.com:9152/\n",
    "1. Organize your screen so that you can see both this notebook as well as the IBM Cloud Pak for Data Console at the same time. This will make it much easier for you to complete the lab without switch back and forth between screens.\n",
    "2. Sign in using your Engineer userid and password\n",
    "3. Click the icon at the very top right of the webpage. It will look something like this:\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.06.10 EngineerUserIcon.png\">\n",
    "\n",
    "4. Click **Profile and settings**\n",
    "5. Click **Permissions** and review the user permissions for this user\n",
    "6. Click the **three bar menu** at the very top left of the console webpage\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/2.42.03 Three Bar.png\">\n",
    "\n",
    "7. Click **Collect** if the Collect menu isn't already open\n",
    "7. Click **Data Virtualization**. The Data Virtualization user interface is displayed\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.06.12 CollectDataVirtualization.png\">\n",
    "\n",
    "8. Click the carrot symbol beside **Menu** below the Data Virtualization title\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/3.07.47 Menu Carrot.png\">\n",
    "\n",
    "This displays the actions available to your user. Different user have access to more or fewer menu options depending on their role in Data Virtualization. \n",
    "\n",
    "As a Data Engineer you can:\n",
    "* Add and modify Data sources. Each source is a connetion to a single database, either inside or outside of IBM Cloud Pak for Data.\n",
    "* Virtualize data. This makes tables in other data sources look and act like tables that are local to the Data Virtualization database\n",
    "* Work with the data you have virtualized.\n",
    "* Write SQL to access and join data that you have virtualized\n",
    "* See detailed in formation on how to connect external analytic tools and applications to your virtualized data\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.12.54 Menu Data sources.png\">\n",
    "\n",
    "As a User you can only:\n",
    "* Work with data that has been virtualized for you\n",
    "* Write SQL to work with that data\n",
    "* See detailed connection information\n",
    "\n",
    "As an Administrator (only available to the course instructor) you can also:\n",
    "* Manage IBM Cloud Pak for Data User Access and Roles\n",
    "* Create and Manage Data Caches to accelerate performance\n",
    "* Change key service setttings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Virtualiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Data Source Connections\n",
    "Let's start by looking at the the Data Source Connections that are already available.    \n",
    "\n",
    "1. Click the Data Virtualization menu and select **Data Sources**.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.12.54 Menu Data sources.png\">\n",
    "\n",
    "2. Click the **icon below the menu with a circle with three connected dots**.\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.14.50 Connections Icons Spider.png\">\n",
    "3. A spider diagram of the connected data sources opens. \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.15.31 Data Sources Spider.png\">\n",
    "\n",
    "    This displays the Data Source Graph with 8 active data sources:\n",
    "    * 4 Db2 Family Databases hosted on premises, IBM Cloud, Azure and AWS\n",
    "    * 1 EDB Postgres Database on Azure\n",
    "    * 1 zOS VSAM file\n",
    "    * 1 Informix Database running on premises \n",
    "**We are not going to add a new data source** but just go through the steps so you can see how to add additional data sources.\n",
    "1. Click **+ Add** at the right of the console screen\n",
    "2. Select **Add data source** from the menu\n",
    "You can see a history of other data source connection information that was used before. This history is maintain to make reconnecting to data sources easier and faster.\n",
    "3. Click **Add connection**\n",
    "4. Click the field below **Connection type**\n",
    "5. Scroll through all the **available data sources** to see the available connection types\n",
    "6. Select **different data connection types** from the list to see the information required to connect to a new data source. \n",
    "At a minumum you typically need the host URL and port address, database name, userid and password. You can also connect using an SSL certificate that can be dragged and dropped directly into the console interface. \n",
    "7. Click **Cancel** to return to the previous list of connections to add\n",
    "8. Click **Cancel** again to return to the list of currently connected data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the available data\n",
    "Now that you understand how to connect to data sources you can start virtualizing data. Much of the work has already been done for you. IBM Cloud Pak for Data searches through the available data sources and compiles a single large inventory of all the tables and data available to virtualize in IBM Cloud Pak for Data. \n",
    "\n",
    "1. Click the Data Virtualization menu and select **Virtualize**\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.07 Menu Virtualize.png\">\n",
    "    \n",
    "2. Check the total number of available tables at the top of the list. There should be well over 500 available.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.15.50 Available Tables.png\">\n",
    "\n",
    "3. Enter \"STOCK\" into the search field and hit **Enter**. Any tables with the string\n",
    "**STOCK** in the tables name, the table schema or with a colunn title that includes **STOCK** will appear in the search results. \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.39.43 Find STOCK.png\">\n",
    "\n",
    "4. Hover your mouse pointer to the far right side to the search results table. A **eye** icon will appear on each row as you move your mouse. \n",
    "5. Click the **eye** icon beside one table. This displays a preview of the data in the selected table.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/3.26.54 Eye.png\">\n",
    "\n",
    "6. Click **X** at the top right of the dialog box to return to the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Tables\n",
    "So that each user in this lab can have their own data to virtualize you will create your own table in a remote database.\n",
    "In this part of the lab you will use this Jupyter notebook and Phyton code to connect to a source database, create a simple table and populate it with data. IBM Cloud Pak for Data will automatically detect the change in the source database and make the new table available for virtualization.\n",
    "\n",
    "In this example we are using Db2 Warehousing running in IBM Cloud Pak for Data but the database can be anywhere. All you need is the connection information and authorized credentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to connect to one of our remote data sources directly as if we were part of the team builing a new business application. Since each lab user will create their own table in their own schema the first thing you need to do is update and run the cell below with your engineer name. \n",
    "1. In this Juypyter notebook, click on the cell below \n",
    "2. Update the engineer name in quotes to match your engineer name\n",
    "3. Click **Run** from the Jupyter notebook menu above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting your userID\n",
    "engineer = 'LABDATAENGINEERx'\n",
    "print('variable engineer set to = ' + str(engineer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the lab relies on a Jupyter notebook extension, commonly refer to as a \"magic\" command, to connect to a Db2 database. To use the commands you load load the extension by running another notebook call db2 that contains all the required code \n",
    "<pre>\n",
    "&#37;run db2.ipynb\n",
    "</pre>\n",
    "The cell below loads the Db2 extension directly from GITHUB. Note that it will take a few seconds for the extension to load, so you should generally wait until the \"Db2 Extensions Loaded\" message is displayed in your notebook. \n",
    "1. Click the cell below\n",
    "2. Click **Run**. When the cell is finished running, In[*] will change to In[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/IBM/db2-jupyter/master/db2.ipynb\n",
    "!wget -O db2.ipynb https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/db2.ipynb\n",
    "\n",
    "%run db2.ipynb\n",
    "print('db2.ipynb loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to Db2\n",
    "\n",
    "Before any SQL commands can be issued, a connection needs to be made to the Db2 database that you will be using. \n",
    "\n",
    "The Db2 magic command tracks whether or not a connection has occured in the past and saves this information between notebooks and sessions. When you start up a notebook and issue a command, the program will reconnect to the database using your credentials from the last session. In the event that you have not connected before, the system will prompt you for all the information it needs to connect. This information includes:\n",
    "\n",
    "- Database name\n",
    "- Hostname\n",
    "- PORT \n",
    "- Userid\n",
    "- Password\n",
    "\n",
    "Run the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to Db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Db2 Warehouse on IBM Cloud Pak for Data Database from inside of IBM Cloud Pak for Data\n",
    "database = 'bludb'\n",
    "user = 'user999'\n",
    "password = 't1cz?K9-X1_Y-2Wi'\n",
    "host = 'openshift-skytap-nfs-woker-5.ibm.com'\n",
    "port = '31928'\n",
    "\n",
    "%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the connection is working. Run the following cell. It lists the tables in the database in the **DVDEMO** schema. Only the first 5 tables are listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select TABNAME, OWNER from syscat.tables where TABSCHEMA = 'DVDEMO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can successfully connect to the database, you are going to create two tables with the same name and column across two different schemas. In following steps of the lab you are going to virtualize these tables in IBM Cloud Paks for Data and fold them together into a single table. \n",
    "\n",
    "The next cell sets the default schema to your engineer name followed by 'A'. Notice how you can set a python variable and substitute it into the SQL Statement in the cell. The **-e** option echos the command. \n",
    "\n",
    "Run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = engineer+'A'\n",
    "%sql -e SET CURRENT SCHEMA {schema}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next cell to create a table with a single INTEGER column containing values from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE DISCOVER (A INT);\n",
    "INSERT INTO DISCOVER VALUES 1,2,3,4,5,6,7,8,9,10;\n",
    "SELECT * FROM DISCOVER;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next two cells to create the same table in a schema ending in **B**. It is populated with values from 11 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = engineer+'B'\n",
    "print(schema)\n",
    "%sql SET CURRENT SCHEMA {schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE DISCOVER (A INT);\n",
    "INSERT INTO DISCOVER VALUES 11,12,13,14,15,16,17,18,19,20;\n",
    "SELECT * FROM DISCOVER;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to see all the tables in the database called **DISCOVER**. You may see tables created by other people running the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT TABSCHEMA, TABNAME FROM SYSCAT.TABLES WHERE TABNAME = 'DISCOVER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtualizing your new Tables\n",
    "Now that you have created two new tables you can virtualize that data and make it look like a single table in your database.\n",
    "1. Return to the IBM Cloud Pak for Data Console\n",
    "2. Click **Virtualize** in the Data Virtualization menu if you are not still in the Virtualize page\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.07 Menu Virtualize.png\">\n",
    "    \n",
    "3. Click the **magnifying glass icon** beside the search bar to refresh the list of available tables\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.15.45 Find tables by name schema column.png\">\n",
    "\n",
    "3. Enter **DISCOVER** in the search bar and hit Enter. Now you can see that your new tables have automatically been discovered by IBM Cloud Pak for Data. You will see your tables listed under the LABDATAENGINEER schemas you used when you created your tables. You will also may see other lab participant tables.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.31.01 Available Discover Tables.png\">\n",
    "\n",
    "4. Select the two tables you just created by clicking the **check box** beside each table. Make sure you only select those for your LABDATAENGINEER schema.\n",
    "5. Click **Add to Cart**. Notice that the number of items in your cart is now **2**.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.31.01 Available Discover Tables.png\">\n",
    "\n",
    "6. Click **View Cart**\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.33.31 View Cart(2).png\">\n",
    "\n",
    "7. Change the name of your two tables from DISCOVER to **DISCOVERA** and **DISCOVERB**. These are the new names that you will be able to use to find your tables in the Data Virtualization database. Don't change the Schema name. It is unique to your current userid. \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.34.21 Assign to Project.png\">\n",
    "    \n",
    "9. Click the **back arrow** beside **Review cart and virtualize tables**. We are going to add one more thing to your cart.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.34.30 Back Arrow Icon.png\">\n",
    "\n",
    "10. Click the checkbox beside **Automatically group tables**. Notice how all the tables called **DISCOVER** have been grouped together into a single entry.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.35.18 Automatically Group Available Tables.png\">\n",
    "\n",
    "11. Select the row were all the DISCOVER table have been grouped together\n",
    "12. Click **Add to cart**. \n",
    "13. Click **View cart**\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.35.28 View cart(3).png\">\n",
    "    \n",
    "    You should now see three items in your cart.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.35.57 Cart with Fold.png\">\n",
    "\n",
    "14. Hover over the elipsis icon at the right side of the list for the **DISCOVER** table\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.34.44 Elipsis.png\">\n",
    "\n",
    "15. Select **Edit grouped tables**\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.36.11 Cart Elipsis Menu.png\">\n",
    "\n",
    "16. Deselect all the tables except for those in one of the schemas you created. You should now have two tables selected. \n",
    "17. Click **Apply**\n",
    "17. Change the name of the new combined table **DISCOVERFOLD**\n",
    "18. Select a project from the drop down list that corresponds to your current user id. \n",
    "19. From the elpsis menu select **Preview** for each of the three tables in your list. The new virtualizaed table **DISCOVERA** should contain values from 1-10. The new virtualized table **DISCOVERB** should contain values from 11-20. The **DISCOVERFOLD** virtualized table should contain values from 1-20.\n",
    "20. Click **Virtualize**. You should that three new virtual tables have been created. \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.36.49 Virtualize.png\">\n",
    "    \n",
    "    The Virtual tables created dialog box opens.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.37.24 Virtual tables created.png\">\n",
    "     \n",
    "21. Click **View my virtualized data**. You return to the My virtualized data page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with your new tables\n",
    "1. Enter **DISCOVER** in the Find field. \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.37.55 Find DISCOVER.png\">\n",
    "    \n",
    "You should see the three virtual tables you just created. Notice that you do not see tables that other users have created. By default, Data Engineers only see virtualized tables they have virtualized or virtual tables where they have been given access by other users. \n",
    "2. Click the elipsis (...) beside your **DISCOVERFOLD** table and select **Preview** to confirm that it contains 20 rows.\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/4.32.01 Elipsis Fold.png\">\n",
    "\n",
    "3. Click **SQL Editor** from the Data Virtualization menu\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.33 Menu SQL editor.png\">\n",
    "\n",
    "4. Click **Blank** to create a new blank SQL Script\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.38.24 + Blank.png\">\n",
    "\n",
    "4. Enter **SELECT * FROM DISCOVERFOLD;** into the SQL Editor\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.38.44 SELECT*.png\">\n",
    "\n",
    "5. Click **Run All** at the bottom left of the SQL Editor window. You should see 20 rows returned in the result. \n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.38.52 Run all.png\">\n",
    "\n",
    "Notice that you didn't have to specify the schema for your new virtual tables. The SQL Editor automatically uses the schema associated with your userid that was used when you created your new tables. \n",
    "\n",
    "Now you can:\n",
    "* Create connection to a remote data source \n",
    "* Make a new or existing table in that remote data source look and act like a local table \n",
    "* Fold data from different tables in the same data source or access data sources by folding it together into a single virtual table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaining Insight from Virtualized Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand the basics of Data Virtualization you can explore how easy it is to gain insight across multiple data sources without moving data. \n",
    "\n",
    "In the next set of steps you connect to virtualized data from from this notebook using your LABDATAENGINEER userid. You can use the same techniques to connect to virtualized data from applications and analytic tools from outside of IBM Cloud Pak for Data. \n",
    "\n",
    "Connecting to all your virtualized data is just like connecting to a single database. All the complexity of a dozens of tables across multiple databases on different on premesis and cloud providers is now as simple as connecting to a single database and querying a table. \n",
    "\n",
    "We are going to connect to the IBM Cloud Pak for Data Virtaulization database in exactly the same way we connected to a Db2 database earlier in this lab. However we need to change the detailed connection information. \n",
    "\n",
    "1. Click **Connection Details** in the Data Virtualization menu\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.44 Menu connection details.png\">\n",
    "\n",
    "2. Click **Without SSL**\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.14.29 Connection details.png\">\n",
    "\n",
    "3. Copy the **User ID** by highlighting it with your mouse, right click and select **Copy**\n",
    "4. Paste the **User ID** in to the next cell in this notebook where **user=** (see below) between the quotation marks\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.54.27 Notebook Login.png\">\n",
    "\n",
    "5. Click **Service Settings** in the Data Virtualization menu\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.14.15 Access information.png\">\n",
    "\n",
    "6. Click **Show** to see the password. Highlight the password and copy using the right-click menu\n",
    "7. Paste the **password** into the cell below between the quotation marks using the righ click paste.\n",
    "8. Run the cell below to connect to the Data Virtualization database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to Data Virtualization SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\n",
    "database = 'bigsql'\n",
    "user = 'userxxxx'\n",
    "password = 'xxxxxxxxxxxxxx'\n",
    "host = 'openshift-skytap-nfs-lb.ibm.com'\n",
    "port = '32080'\n",
    "\n",
    "%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Symbol Table\n",
    "#### Get information about the stocks that are in the database\n",
    "**System Z - VSAM**\n",
    "This table comes from a VSAM file on zOS. IBM Cloud Pak for Data Virtualization works together with Data Virtualization Manager for zOS to make this looks like a local database table. For the following examples you can substitute any of the symbols below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql -grid select * from DVDEMO.STOCK_SYMBOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock History Table\n",
    "#### Get Price of a Stock over the Year\n",
    "Set the Stock Symbol in the line below and run the cell. This informaiton is folded together with data coming from two identical tables, one on Db2 database and on on and Informix database. Run the next two cells. Then pick a new stock symbol from the list above, enter it into the cell below and run both cells again.\n",
    "\n",
    "**CP4D - Db2, Skytap -  Informix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = 'AXP'\n",
    "print('variable stock set to = ' + str(stock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -pl\n",
    "SELECT WEEK(TX_DATE) AS WEEK, OPEN FROM FOLDING.STOCK_HISTORY\n",
    "WHERE SYMBOL = :stock AND TX_DATE != '2017-12-01'\n",
    "ORDER BY WEEK(TX_DATE) ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend of Three Stocks\n",
    "This chart shows three stock prices over the course of a year. It uses the same folded stock history information.\n",
    "\n",
    "**CP4D - Db2, Skytap -  Informix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ['INTC','MSFT','AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -pl\n",
    "SELECT SYMBOL, WEEK(TX_DATE), OPEN FROM FOLDING.STOCK_HISTORY\n",
    "WHERE SYMBOL IN (:stocks) AND TX_DATE != '2017-12-01'\n",
    "ORDER BY WEEK(TX_DATE) ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 Day Moving Average of a Stock\n",
    "Enter the Stock Symbol below to see the 30 day moving average of a single stock.\n",
    "\n",
    "**CP4D - Db2, Skytap -  Informix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = 'AAPL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlin = \\\n",
    "\"\"\"\n",
    "SELECT WEEK(TX_DATE) AS WEEK, OPEN, \n",
    "     AVG(OPEN) OVER (\n",
    "       ORDER BY TX_DATE\n",
    "     ROWS BETWEEN 15 PRECEDING AND 15 FOLLOWING) AS MOVING_AVG\n",
    "  FROM FOLDING.STOCK_HISTORY\n",
    "     WHERE SYMBOL = :stock\n",
    "  ORDER BY WEEK(TX_DATE)\n",
    "\"\"\"\n",
    "df = %sql {sqlin}\n",
    "txdate= df['WEEK']\n",
    "sales = df['OPEN']\n",
    "avg = df['MOVING_AVG']\n",
    "\n",
    "plt.xlabel(\"Day\", fontsize=12);\n",
    "plt.ylabel(\"Opening Price\", fontsize=12);\n",
    "plt.suptitle(\"Opening Price and Moving Average of \" + stock, fontsize=20);\n",
    "plt.plot(txdate, sales, 'r');\n",
    "plt.plot(txdate, avg, 'b');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trading volume of INTC versus MSFT and AAPL in first week of November\n",
    "**CP4D - Db2, Skytap -  Informix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ['INTC','MSFT','AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -pb\n",
    "SELECT SYMBOL, DAY(TX_DATE), VOLUME/1000000 FROM FOLDING.STOCK_HISTORY\n",
    "WHERE SYMBOL IN (:stocks) AND WEEK(TX_DATE) =  45\n",
    "ORDER BY DAY(TX_DATE) ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Stocks that Represent at least 3% of the Total Purchases during Week 45\n",
    "**CP4D - Db2, Skytap -  Informix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -pie\n",
    "WITH WEEK45(SYMBOL, PURCHASES) AS (\n",
    "  SELECT SYMBOL, SUM(VOLUME * CLOSE) FROM FOLDING.STOCK_HISTORY\n",
    "    WHERE WEEK(TX_DATE) =  45 AND SYMBOL <> 'DJIA'\n",
    "  GROUP BY SYMBOL\n",
    "),\n",
    "ALL45(TOTAL) AS (\n",
    "  SELECT SUM(PURCHASES) * .03 FROM WEEK45\n",
    ")\n",
    "SELECT SYMBOL, PURCHASES FROM WEEK45, ALL45\n",
    "WHERE PURCHASES > TOTAL\n",
    "ORDER BY SYMBOL, PURCHASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Transaction Table\n",
    "#### Show Transactions by Customer\n",
    "This next two examples uses data folded together from three different data sources representing three different trading organizations to create a combined of a single customer's stock trades. \n",
    "\n",
    "**AWS - Db2, Azure - EDB (Postgres), Azure - Db2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM FOLDING.STOCK_TRANSACTIONS\n",
    " WHERE CUSTID = '107196'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bought/Sold Amounts of Top 5 stocks \n",
    "**AWS - Db2, Azure - EDB (Postgres), Azure - Db2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "WITH BOUGHT(SYMBOL, AMOUNT) AS\n",
    "  (\n",
    "  SELECT SYMBOL, SUM(QUANTITY) FROM FOLDING.STOCK_TRANSACTIONS\n",
    "  WHERE QUANTITY > 0\n",
    "  GROUP BY SYMBOL\n",
    "  ),\n",
    "SOLD(SYMBOL, AMOUNT) AS\n",
    "  (\n",
    "  SELECT SYMBOL, -SUM(QUANTITY) FROM FOLDING.STOCK_TRANSACTIONS\n",
    "  WHERE QUANTITY < 0\n",
    "  GROUP BY SYMBOL\n",
    "  )\n",
    "SELECT B.SYMBOL, B.AMOUNT AS BOUGHT, S.AMOUNT AS SOLD\n",
    "FROM BOUGHT B, SOLD S\n",
    "WHERE B.SYMBOL = S.SYMBOL\n",
    "ORDER BY B.AMOUNT DESC\n",
    "FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Accounts\n",
    "#### Show Top 5 Customer Balance\n",
    "These next two examples use data folded from systems running on AWS and Azure.\n",
    "**AWS - Db2, Azure - EDB (Postgres), Azure - Db2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT CUSTID, BALANCE FROM FOLDING.ACCOUNTS\n",
    "ORDER BY BALANCE DESC\n",
    "FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Bottom 5 Customer Balance\n",
    "**AWS - Db2, Azure - EDB (Postgres), Azure - Db2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT CUSTID, BALANCE FROM FOLDING.ACCOUNTS\n",
    "ORDER BY BALANCE ASC\n",
    "FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Customer Information from MongoDB\n",
    "The MongoDB database (running on premises) has customer information in a document format. In order to materialize the document data as relational tables, a total of four virtual tables are generated. The following query shows the tables that are generated for the Customer document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql LIST TABLES FOR SCHEMA MONGO_ONPREM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables are all connected through the CUSTOMERID field, which is based on the generated _id of the main CUSTOMER colllection. In order to reassemble these tables into a document, we must join them using this unique identifier. An example of the contents of the CUSTOMER_CONTACT table is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql -grid SELECT * FROM MONGO_ONPREM.CUSTOMER_CONTACT FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full document record is shown in the following SQL statement which joins all of the tables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -grid\n",
    "SELECT C.CUSTOMERID AS CUSTID, \n",
    "       CI.FIRSTNAME, CI.LASTNAME, CI.BIRTHDATE,\n",
    "       CC.CITY, CC.ZIPCODE, CC.EMAIL, CC.PHONE, CC.STREET, CC.STATE,\n",
    "       CP.CARD_TYPE, CP.CARD_NO\n",
    "FROM MONGO_ONPREM.CUSTOMER C, MONGO_ONPREM.CUSTOMER_CONTACT CC, \n",
    "     MONGO_ONPREM.CUSTOMER_IDENTITY CI, MONGO_ONPREM.CUSTOMER_PAYMENT CP\n",
    "WHERE  CC.CUSTOMER_ID = C.\"_ID\" AND\n",
    "       CI.CUSTOMER_ID = C.\"_ID\" AND\n",
    "       CP.CUSTOMER_ID = C.\"_ID\"\n",
    "FETCH FIRST 3 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying All Virtualized Data\n",
    "In this final example we use data from each data source to answer a complex business question. \"What are the names of the customers in Ohio, who bought the most during the highest trading day of the year (based on the Dow Jones Industrial Index)?\" \n",
    "\n",
    "**AWS Db2, Azure EDB, Azure Db2, Skytap MongoDB, CP4D Db2Wh, Skytap Informix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "WITH MAX_VOLUME(AMOUNT) AS (\n",
    "  SELECT MAX(VOLUME) FROM FOLDING.STOCK_HISTORY\n",
    "    WHERE SYMBOL = 'DJIA'\n",
    "),\n",
    "HIGHDATE(TX_DATE) AS (\n",
    "  SELECT TX_DATE FROM FOLDING.STOCK_HISTORY, MAX_VOLUME M\n",
    "    WHERE SYMBOL = 'DJIA' AND VOLUME = M.AMOUNT\n",
    "),\n",
    "CUSTOMERS_IN_OHIO(CUSTID) AS (\n",
    "  SELECT C.CUSTID FROM TRADING.CUSTOMERS C \n",
    "    WHERE C.STATE = 'OH'\n",
    "),\n",
    "TOTAL_BUY(CUSTID,TOTAL) AS (\n",
    "  SELECT C.CUSTID, SUM(SH.QUANTITY * SH.PRICE) \n",
    "    FROM CUSTOMERS_IN_OHIO C, FOLDING.STOCK_TRANSACTIONS SH, HIGHDATE HD\n",
    "  WHERE SH.CUSTID = C.CUSTID AND\n",
    "        SH.TX_DATE = HD.TX_DATE AND \n",
    "        QUANTITY > 0 \n",
    "  GROUP BY C.CUSTID\n",
    ")\n",
    "  SELECT LASTNAME, T.TOTAL\n",
    "  FROM MONGO_ONPREM.CUSTOMER_IDENTITY CI, MONGO_ONPREM.CUSTOMER C, TOTAL_BUY T\n",
    "  WHERE CI.CUSTOMER_ID = C.\"_ID\" AND C.CUSTOMERID = CUSTID\n",
    "  ORDER BY TOTAL DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing where your Virtualized Data is coming from\n",
    "You make eventually work with a complex Data Virtualization system. As an administrator or a Data Scientist you may need to understand where data is coming from. \n",
    "\n",
    "Fortunately, the Data Virtualization engine is based on Db2. It includes the same catalog of information as does Db2 with some additional features. If you want to work backwards and understand where each of your virtualized tables comes from, the information is included in the **SYSCAT.TABOPTIONS** catalog table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT TABSCHEMA, TABNAME, SETTING\n",
    "  FROM SYSCAT.TABOPTIONS\n",
    "  WHERE OPTION = 'SOURCELIST' \n",
    "    AND TABSCHEMA <> 'QPLEXSYS';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT * from SYSCAT.TABOPTIONS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table includes more information than you need to answer the question of where is my data coming from. The query below only shows the rows that contain the information of the source of the data ('SOURCELIST'). Notice that tables that have been folded together from several tables includes each of the data source information seperated by a semi-colon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT TABSCHEMA, TABNAME, SETTING\n",
    "  FROM SYSCAT.TABOPTIONS\n",
    "  WHERE OPTION = 'SOURCELIST' \n",
    "    AND TABSCHEMA <> 'QPLEXSYS';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT TABSCHEMA, TABNAME, SETTING\n",
    "  FROM SYSCAT.TABOPTIONS\n",
    "  WHERE TABSCHEMA = 'DVDEMO';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, you can search for any virtualized data coming from a Postgres database by searching for **SETTING LIKE '%POST%'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT TABSCHEMA, TABNAME, SETTING\n",
    "  FROM SYSCAT.TABOPTIONS\n",
    "  WHERE OPTION = 'SOURCELIST' \n",
    "    AND SETTING LIKE '%POST%'\n",
    "    AND TABSCHEMA <> 'QPLEXSYS';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is missing is additional detail for each connection. For example all we can see in the table above is a connection. You can find that detail in another table: **QPLEXSYS.LISTRDBC**. In the last cell, you can see that CID DB210113 is included in the STOCK_TRANSACTIONS virtual table. You can find the details on that copy of Db2 by running the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT CID, USR, SRCTYPE, SRCHOSTNAME, SRCPORT, DBNAME, IS_DOCKER FROM QPLEXSYS.LISTRDBC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Virtualization \n",
    "Now that you have seen how powerful and easy it is to gain insight from your existing virtualized data, you can learn more about how to do advanced data virtualization. You will learn how to join different remote tables together to create a new virtual table and how to capture complex SQL into VIEWs.\n",
    "\n",
    "\n",
    "### Joining Tables Together\n",
    "The virtualized tables below come from different data sources on different systems. We can combine them into a single virtual table. \n",
    "\n",
    "* Select **My virtualized data** from the Data Virtualization menu\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.20 Menu My virtual data.png\">\n",
    "  \n",
    "* Enter **Stock** in the find field and hit enter\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.39.43 Find STOCK.png\">\n",
    "  \n",
    "* Select table **STOCK_TRANSACTIONS** in the **FOLDING** schema\n",
    "* Select table **STOCK_SYMBOLS** in the **DVDEMO** schema\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.40.18 Two STOCK seleted.png\">\n",
    "  \n",
    "* Click **Join View**\n",
    "* In table STOCK_SYMBOLS: deselect **SYMBOL**\n",
    "* In table STOCK_TRANSACTIONS: deselect **TX_NO** \n",
    "* Click **STOCK_TRANSACTION.SYMBOL** and drag to **STOCK_SYMBOLS.SYMBOL**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.41.07 Joining Tables.png\">\n",
    "  \n",
    "* Click **Preview** to check that your join is working. Each row shoud now contain the stock symbol and the long stock name.\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.41.55 New Join Preview.png\">\n",
    "  \n",
    "* Click **X** to close the preview window\n",
    "* Click **JOIN**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.42.20 Join.png\">\n",
    "  \n",
    "* Type view name **TRANSACTIONS_FULLNAME**\n",
    "* Don't change the default schema. This corresponds to your LABENGINEER user id. \n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.43.10 View Name.png\">\n",
    "  \n",
    "* Click **NEXT**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.43.30 Next.png\">\n",
    "  \n",
    "* Select the lab **project** that corresponds to your LABENGINEER id. \n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.43.58 Assign to Project.png\">\n",
    "  \n",
    "* Click **CREATE VIEW**. \n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.44.06 Create view.png\">\n",
    "  \n",
    "  You see the successful Join View window.\n",
    "  \n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.44.23 Join view created.png\">\n",
    "  \n",
    "  \n",
    "* Click **View my virtualized data**\n",
    "* Click the elipsis menu beside **TRANSACTIONS_FULLNAME**\n",
    "* Click **Preview**\n",
    "\n",
    "You can now join virtualize tables together to combine them into new virtualized tables. Now that you know how to perform simple table joins you can learn how to combine multiple data sources and virtual tables using the powerful SQL query engine that is part of the IBM Cloud Pak for Data - Virtualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Queries to Answer Complex Business Questions\n",
    "The IBM Cloud Pak for Data Virtualization Administrator has set up more complex data from multiple source for the next steps. The administrator has also given you access to this virtualized data. You may have noticed this in previous steps. \n",
    "1. Select **My virtualized data** from the Data Virtualiztion menu. All of these virtualized tables look and act like normal Db2 tables. \n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.20 Menu My virtual data.png\">\n",
    " \n",
    "2. Click **Preview** for any of the tables to see what they contain. \n",
    "\n",
    "The virtualized tables in the **FOLDING** schema have all been created by combinig the same tables from different data source. Folding isn't something that is restricted to the same data source in the simple example you just completed.\n",
    "\n",
    "The virtaulized tables in the **TRADING** schema are view of complex queries that were use to combine data from multiple data source to answer specific business questions. \n",
    "\n",
    "3. Select **SQL Editor** from the Data Virtualization menu.\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.33 Menu SQL editor.png\">\n",
    "\n",
    "4. Select **Script Library**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.45.02 Script Library.png\">\n",
    "\n",
    "5. Search for **OHIO**\n",
    "6. Select and expand the **OHIO Customer** query\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.45.47 Ohio Script.png\">\n",
    "\n",
    "7. Click the **Open a script to edit** icon to open the script in the SQL Editor\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.45.54 Open Script.png\">\n",
    "\n",
    "8. Click **Run All**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.46.21 Run Ohio Script.png\">\n",
    "\n",
    "\n",
    "This script is a complex SQL join query that uses data from all the virtualize data sources you explored in the first steps of this lab. While the SQL looks complex the author of the query did not have be aware that the data was coming from multiple sources. Everything used in this query looks like it comes from a single database, not eight different data sources across eight different systems on premesis or in the Cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Complex SQL Simple to Consume\n",
    "You can easily make this complex query easy for a user to consume. Instead of shaing this query with other users, you can wrap the query into a view that looks and acts like a simple table. \n",
    "1. Enter **CREATE VIEW MYOHIOQUERY AS** in the SQL Editor at the first line below the comment and before the **WITH** clause\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.46.54 Add CREATE VIEW.png\">\n",
    "\n",
    "2. Click **Run all**\n",
    "3. Click **+** to **Add a new script**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.48.28 Add to script.png\">\n",
    "  \n",
    "4. Click **Blank**\n",
    "4. Enter **SELECT * FROM MYOHIOQUERY;**\n",
    "5. Click **Run all**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.48.57 Run Ohio View.png\">\n",
    "  \n",
    "\n",
    "Now you have a very simple virtualized table that is pulling data from eight different data sources, combining the data together to resolve a complex business problem. In the next step you will share your new virtualized data with a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing Virtualized Tables\n",
    "1. Select **My virtualized data** from the Data Virtualization Menu.\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.13.20 Menu My virtual data.png\">\n",
    " \n",
    "2. Click the elipsis (...) menu to the right of the **MYOHIOQUERY** virtualized table\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.49.30 Select MYOHIOQUERY.png\">\n",
    "  \n",
    "3. Select **Manage Access** from the elipsis menu\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.49.46 Virtualized Data Menu.png\">\n",
    " \n",
    "3. Click **Grant access**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.50.07 Grant access.png\">\n",
    " \n",
    "4. Select the **LABUSERx** id associated with your lab. For example, if you are LABDATAENGINEER5, then select LABUSER5.\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.52.42 Grant access to specific user.png\">\n",
    " \n",
    "5. Click **Add**\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/11.50.28 Add.png\">\n",
    " \n",
    "\n",
    "You should now see that your **LABUSER** id has view only access to the new virtualized table. Nextyou switch to your LABUSERx id to check that you can see the data you have just granted access for.\n",
    "\n",
    "6. Click the user icon at the very top right of the console\n",
    "7. Click **Log out**\n",
    "8. Sign in using the LABUSER id specified by your lab instructor\n",
    "9. Click the three bar menu at the top left of the IBM Cloud Pak for Data console\n",
    "10. Select **Data Virtualization**\n",
    "\n",
    "You should see the **MYOHIOQUERY** with the schema from your engineer userid in the list of virtualized data.\n",
    "\n",
    "11. Make a note of the schema of the MYOHIOQUERY in your list of virtualized tables. It starts with **USER**.\n",
    "12. Select the **SQL Editor** from the Data virtualization menu\n",
    "13. Click **Blank** to open a new SQL Editor window\n",
    "14. Enter **SELECT * FROM USERxxxx.MYOHIOQUERY** where xxxx is the user number of your engineer user. The view created by your engineer user was created in their default schema. \n",
    "15. Click **Run all**\n",
    "16. Add the following to your query: **WHERE TOTAL > 3000 ORDER BY TOTAL**\n",
    "17. Click **</>** to format the query so it is easiler to read\n",
    "18. Click **Run all**\n",
    "\n",
    "You can see how you have just make a very complex data set extremely easy to consume by a data user. They don't have to know how to connect to multiple data sources or how to combine the data using complex SQL. You can hide that complexity while ensuring only the right user has access to the right data. \n",
    "\n",
    "In the next steps you will learn how to access virtualized data from outside of IBM Cloud Pak for Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowing User to Access Virtualized Data with Analytic Tools\n",
    "In the next set of steps you connect to virtualized data from from this notebook using your **LABUSER** userid. \n",
    "\n",
    "Just like you connected to IBM Cloud Pak for Data Virtualized Data using your LABDATAENGINEER you can connect using your LABUSER. \n",
    "\n",
    "We are going to connect to the IBM Cloud Pak for Data Virtualization database in exactly the same way we connected using you LABENGINEER. However you need to change the detailed connection information. Each user has their own unique userid and password to connect to the service. This ensures that no matter what tool you use to connect to virtualized data you are always in control over who can access specifical virtualized data. \n",
    "\n",
    "2. Click the user icon at the top right of the IBM Cloud Pak for data console to confirm that you are using your **LABUSER** id\n",
    "1. Click **Connection Details** in the Data Virtualization menu\n",
    "2. Click **Without SSL**\n",
    "3. Copy the **User ID** by highlighting it with your mouse, right click and select **Copy**\n",
    "4. Paste the **User ID** in to the cell below were **user =** between the quotation marks \n",
    "5. Click **Service Settings** in the Data Virtualization menu\n",
    "6. Show the password. Highlight the password and copy using the right click menu\n",
    "7. Paste the **password** into the cell below between the quotation marks using the righ click paste.\n",
    "8. Run the cell below to connect to the Data Virtualization database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting a USER to Data Virtualization SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\n",
    "database = 'bigsql'\n",
    "user = 'userxxxx'\n",
    "password = 'xxxxxxxxxxxxxxxxxx'\n",
    "host = 'openshift-skytap-nfs-lb.ibm.com'\n",
    "port = '32080'\n",
    "\n",
    "%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try out the view that was created by the LABDATAENGINEER userid. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM {user}.MYOHIOQUERY WHERE TOTAL > 3000 ORDER BY TOTAL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only LABENGINEER virtualized tables that have been authorized for the LABUSER to see are available. Try running the next cell. You should receive an error that the current user does not have the required authorization or privlege to perform the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM {user}.DISCOVERFOLD;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "Now you can use IBM Cloud Pak for Data to make even complex data and queries from different data sources, on premesis and across a multi-vendor Cloud look like simple tables in a single database. You are ready for some more advanced labs. \n",
    "\n",
    "1. Use Db2 SQL and Jupyter Notebooks to Analyze Virtualized Data\n",
    "    * Build simple to complex queries to answer important business questions using the virtualized data available to you in IBM Cloud Pak for Data\n",
    "    * See how you can transform the queries into simple tables available to all your users\n",
    "2. Use Open RESTful Services to connect to the IBM Cloud Pak for Data Virtaulization \n",
    "    * Everything you can do in the IBM Cloud Pak for Data User Interface is accessible through Open RESTful APIs\n",
    "    * Learn how to automate and script your managment of Data Virtualization using RESTful API\n",
    "    * Learn how to accelerate appliation development by accessing virtaulied data through RESTful APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Data Virtualization Setup and Management through REST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IBM Cloud Pak for Data Console is only one way you can interact with the Virtualization service. IBM Cloud Pak for Data is built on set of microservice that communicate with each other and the Console user interface using RESTful APIs. You can use these services to automate anything you can do throught the user interface.\n",
    "\n",
    "This Jupyter Notebook contains examples of how to use the Open APIs to retrieve information from the virtualization service, how to run SQL statements directly against the service through REST and how to provide authoritization to objects. This provides a way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "The next part of the lab relies on a set of base classes to help you interact with the RESTful Services API for IBM Cloud Pak for Data Virtualization. You can access this library on GITHUT. The commands below download the library and run them as part of this notebook.\n",
    "<pre>\n",
    "&#37;run CPDDVRestClass.ipynb\n",
    "</pre>\n",
    "The cell below loads the RESTful Service Classes and methods directly from GITHUB. Note that it will take a few seconds for the extension to load, so you should generally wait until the \"Db2 Extensions Loaded\" message is displayed in your notebook. \n",
    "1. Click the cell below\n",
    "2. Click **Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O CPDDVRestClass.ipynb https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/CPDDVRestClass.ipynb\n",
    "%run CPDDVRestClass.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Db2 Class\n",
    "The CPDDVRestClass.ipynb notebook includes a Python class called Db2 that encapsulates the Rest API calls used to connect to the IBM Cloud Pak for Data Virtualization service. \n",
    "\n",
    "To access the service you need to first authenticate with the service and create a reusable token that we can use for each call to the service. This ensures that we don't have to provide a userID and password each time we run a command. The token makes sure this is secure. \n",
    "\n",
    "Each request is constructed of several parts. First, the URL and the API identify how to connect to the service. Second the REST service request that identifies the request and the options. For example '/metrics/applications/connections/current/list'. And finally some complex requests also include a JSON payload. For example running SQL includes a JSON object that identifies the script, statement delimiters, the maximum number of rows in the results set as well as what do if a statement fails.\n",
    "\n",
    "You can find this class and use it for your own notebooks in GITHUB. Have a look at how the class encapsulated the API calls by clicking on the following link: https://github.com/Db2-DTE-POC/CPDDVLAB/blob/master/CPDDVRestClass.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Connections\n",
    "To connect to the Data Virtualization service you need to provide the URL, the service name (v1) and profile the console user name and password. For this lab we are assuming that the following values are used for the connection:\n",
    "* Userid: LABDATAENGINEERx\n",
    "* Password: password\n",
    "\n",
    "Substitute your assigned LABDATAENGINEER userid below along with your password and run the cell. It will generate a breaer token that is used in the following steps to authenticate your use of the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to Data Virtualization API Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the service URL to connect from inside the ICPD Cluster\n",
    "Console  = 'https://openshift-skytap-nfs-lb.ibm.com'\n",
    "\n",
    "# Connect to the Db2 Data Management Console service\n",
    "user     = 'labdataengineerx'\n",
    "password = 'password'\n",
    "\n",
    "# Set up the required connection\n",
    "databaseAPI = Db2(Console)\n",
    "api = '/v1'\n",
    "databaseAPI.authenticate(api, user, password)\n",
    "database = Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sources\n",
    "The following call (getDataSources) uses an SQL call in the DB2 class to run the same SQL statement you saw earlier in the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Available Data Sources already configured\n",
    "json = databaseAPI.getDataSources()\n",
    "databaseAPI.displayResults(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virtualized Data\n",
    "This call retrieves all of the virtualized data available to the role of Data Engineer. It uses a direct RESTful service call and does not use SQL. The service returns a JSON result set that is converted into a Python Pandas dataframe. Dataframes are very useful in being able to manipulate tables of data in Python. If there is a problem with the call, the error code is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Virtualized Assets Avalable to Engineers\n",
    "roles = ['DV_ENGINEER']\n",
    "for role in roles:\n",
    "    r = databaseAPI.getRole(role)\n",
    "    if (databaseAPI.getStatusCode(r)==200):\n",
    "        json = databaseAPI.getJSON(r)\n",
    "        df = pd.DataFrame(json_normalize(json['objects']))\n",
    "        display(df)\n",
    "    else:\n",
    "        print(databaseAPI.getStatusCode(r))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virtualized Tables and Views\n",
    "This call retrieves all the virtualized tables and view available to the userid that you use to connect to the service. In this example the whole call is included in the DB2 class library and returned as a complete Dataframe ready for display or to be used for analysis or administration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display Virtualized Tables and Views \n",
    "display(databaseAPI.getVirtualizedTablesDF())\n",
    "display(databaseAPI.getVirtualizedViewsDF())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of the IBM Cloud Pak for Data Users\n",
    "This example returns a list of all the users of the IBM Cloud Pak for Data system. It only displays three colunns in the Dataframe, but the list of all the available columns is als printed out. Try changing the code to display other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of CPD Users\n",
    "r = databaseAPI.getUsers()\n",
    "if (databaseAPI.getStatusCode(r)==200):\n",
    "    json = databaseAPI.getJSON(r)\n",
    "    df = pd.DataFrame(json_normalize(json))\n",
    "    print(', '.join(list(df))) # List available column names\n",
    "    display(df[['uid','username','displayName']])\n",
    "else:\n",
    "    print(databaseAPI.getStatusCode(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of available schemas in the DV Database\n",
    "Do not forget that the Data Virtualization engine supports the same function as a regular Db2 database. So you can also look at standard Db2 objects like schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of available schemas in the DV Database\n",
    "r = databaseAPI.getSchemas()\n",
    "if (databaseAPI.getStatusCode(r)==200):\n",
    "    json = databaseAPI.getJSON(r)\n",
    "    df = pd.DataFrame(json_normalize(json['resources']))\n",
    "    print(', '.join(list(df)))\n",
    "    display(df[['name']].head(10))\n",
    "else:\n",
    "    print(databaseAPI.getStatusCode(r))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Search\n",
    "Fuzzy object search is also available. The call is a bit more complex. If you look at the routine in the DB2 class it posts a RESTful service call that includes a JSON payload. The payload includes the details of the search request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for tables across all schemas that match simple search critera \n",
    "# Display the first 100\n",
    "# Switch between searching tables or views\n",
    "object = 'view'\n",
    "# object = 'table'\n",
    "r = databaseAPI.postSearchObjects(object,\"TRADING\",10,'false','false')\n",
    "if (databaseAPI.getStatusCode(r)==200):\n",
    "    json = databaseAPI.getJSON(r)\n",
    "    df = pd.DataFrame(json_normalize(json))\n",
    "    print('Columns:')\n",
    "    print(', '.join(list(df)))\n",
    "    display(df[[object+'_name']].head(100))\n",
    "else:\n",
    "    print(\"RC: \"+str(databaseAPI.getStatusCode(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SQL through the SQL Editor Service\n",
    "You can also use the SQL Editor service to run your own SQL. Statements are submitted to the editor. Your code then needs to poll the editor service until the script is complete. Fortunately you can use the DB2 class included in this lab so that it becomes a very simple Python call. The **runScript** routine runs the SQL and the **displayResults** routine formats the returned JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlText = \\\n",
    "'''\n",
    "WITH MAX_VOLUME(AMOUNT) AS (\n",
    "  SELECT MAX(VOLUME) FROM FOLDING.STOCK_HISTORY\n",
    "    WHERE SYMBOL = 'DJIA'\n",
    "),\n",
    "HIGHDATE(TX_DATE) AS (\n",
    "  SELECT TX_DATE FROM FOLDING.STOCK_HISTORY, MAX_VOLUME M\n",
    "    WHERE SYMBOL = 'DJIA' AND VOLUME = M.AMOUNT\n",
    "),\n",
    "CUSTOMERS_IN_OHIO(CUSTID) AS (\n",
    "  SELECT C.CUSTID FROM TRADING.CUSTOMERS C \n",
    "    WHERE C.STATE = 'OH'\n",
    "),\n",
    "TOTAL_BUY(CUSTID,TOTAL) AS (\n",
    "  SELECT C.CUSTID, SUM(SH.QUANTITY * SH.PRICE) \n",
    "    FROM CUSTOMERS_IN_OHIO C, FOLDING.STOCK_TRANSACTIONS SH, HIGHDATE HD\n",
    "  WHERE SH.CUSTID = C.CUSTID AND\n",
    "        SH.TX_DATE = HD.TX_DATE AND \n",
    "        QUANTITY > 0 \n",
    "  GROUP BY C.CUSTID\n",
    ")\n",
    "  SELECT LASTNAME, T.TOTAL\n",
    "  FROM MONGO_ONPREM.CUSTOMER_IDENTITY CI, MONGO_ONPREM.CUSTOMER C, TOTAL_BUY T\n",
    "  WHERE CI.CUSTOMER_ID = C.\"_ID\" AND C.CUSTOMERID = CUSTID\n",
    "  ORDER BY TOTAL DESC\n",
    "FETCH FIRST 5 ROWS ONLY;\n",
    "'''\n",
    "\n",
    "databaseAPI.displayResults(databaseAPI.runScript(sqlText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run scripts of SQL Statements repeatedly through the SQL Editor Service\n",
    "The runScript routine can contain more than one statement. The next example runs a scipt with eight SQL statements multple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 3\n",
    "sqlText = \\\n",
    "'''\n",
    "SELECT * FROM TRADING.MOVING_AVERAGE;\n",
    "SELECT * FROM TRADING.VOLUME;\n",
    "SELECT * FROM TRADING.THREEPERCENT;\n",
    "SELECT * FROM TRADING.TRANSBYCUSTOMER;\n",
    "SELECT * FROM TRADING.TOPBOUGHTSOLD;\n",
    "SELECT * FROM TRADING.TOPFIVE;\n",
    "SELECT * FROM TRADING.BOTTOMFIVE;\n",
    "SELECT * FROM TRADING.OHIO;\n",
    "'''\n",
    "\n",
    "for x in range(0, repeat):\n",
    "    print('Repetition number: '+str(x))\n",
    "    databaseAPI.displayResults(databaseAPI.runScript(sqlText))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next\n",
    "if you are inteested in finding out more about using RESTful services to work with Db2, check out this DZone article: https://dzone.com/articles/db2-dte-pocdb2dmc. The article also includes a link to a complete hands-on lab for Db2 and the Db2 Data Management Console. In it you can find out more about using REST and Db2 together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up at the end of your lab\n",
    "Like any good student we as you to clean up your workspace at the end of the lab. Please connect to the Db2 Database were you created new tables and drop them using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Db2 Warehouse on IBM Cloud Pak for Data Database from inside of IBM Cloud Pak for Data\n",
    "database = 'bludb'\n",
    "user = 'user999'\n",
    "password = 't1cz?K9-X1_Y-2Wi'\n",
    "host = 'openshift-skytap-nfs-woker-5.ibm.com'\n",
    "port = '31928'\n",
    "\n",
    "%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Db2 Warehouse on IBM Cloud Pak for Data Database from outside of IBM Cloud Pak for Data\n",
    "\n",
    "database = 'bludb'\n",
    "user = 'user999'\n",
    "password = 't1cz?K9-X1_Y-2Wi'\n",
    "host = 'services-uscentral.skytap.com'\n",
    "port = '9094'\n",
    "\n",
    "%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = engineer+'A'\n",
    "%sql DROP TABLE {schema}.DISCOVER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = engineer+'B'\n",
    "%sql DROP TABLE {schema}.DISCOVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP TABLE DATAENGINEER1A.DISCOVER1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM SYSCAT.TABLES WHERE TABNAME LIKE '%DISCOVER%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits: IBM 2019, Peter Kohlmann [kohlmann@ca.ibm.com]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
