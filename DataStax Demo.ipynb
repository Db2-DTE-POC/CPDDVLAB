{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2517b27f-652a-48a4-8329-1850baa5d575"
   },
   "source": [
    "# IBM Cloud Pak for Data - DataStax Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9488e5e-815a-47a0-b278-cd203645fc20"
   },
   "source": [
    "## Introduction\n",
    "Welcome to the IBM Cloud Pak for Data Multi-Cloud Virtualization Hands on Lab. \n",
    "\n",
    "In this lab you analyze data from multiple data sources, from across multiple Clouds, without copying data into a warehouse.\n",
    "\n",
    "This hands-on lab uses live databases, were data is “virtually” available through the IBM Cloud Pak for Data Virtualization Service. This makes it easy to analyze data from across your multi-cloud enterprise using tools like, Jupyter Notebooks, Watson Studio or your favorite reporting tool like Cognos.  \n",
    "\n",
    "Take a minute to watch this introductory video to get an overview of what you will see in this live Hands on Lab system.\n",
    "http://ibm.biz/DTEVirtualMultiCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be5b0f97-e4d9-4eeb-be7d-be752b22f1c2"
   },
   "source": [
    "### Where to find this sample online\n",
    "You can find a copy of this notebook on GITHUB at https://github.com/Db2-DTE-POC/CPDDVLAB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bafbbeee-501f-49c2-acb6-f637c9f82313"
   },
   "source": [
    "### The business problem and the landscape\n",
    "The Acme Company needs timely analysis of stock trading data from multiple source systems. \n",
    "\n",
    "Their data science and development teams needs access to:\n",
    "* Customer data\n",
    "* Account data\n",
    "* Trading data\n",
    "* Stock history and Symbol data\n",
    "\n",
    "The data sources are running on premises and on the cloud. In this example many of the databases are also running on OpenShift but they could be managed, virtual or bare-metal cloud installations. IBM Cloud Pak for Data doesn't care. Enterprise DB (Postgres) is also running in the Cloud. Mongo and Informix are running on premises. \n",
    "\n",
    "To simplify access for Data Scientists and Developers the Acme team wants to make all their data look like it is coming from a single database. They also want to combine data to create simple to use tables.\n",
    "\n",
    "In the past, Acme built a dedicated data warehouse, and then created ETL (Export, Transform and Load) job to move data from each data source into the warehouse were it could be combined. Now they can just virtualize your data without moving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf38b959-3056-42f2-9952-162996d4e4d0"
   },
   "source": [
    "### Loading the Python Libraries\n",
    "To submit SQL statements to Data Virtualization relies we use a Jupyter notebook extension, commonly refer to as a \"magic\" command, to connect to a Db2 database. To use the commands you load load the extension by running another notebook call db2 that contains all the required code \n",
    "<pre>\n",
    "&#37;run db2.ipynb\n",
    "</pre>\n",
    "The cell below loads the Db2 extension directly from GITHUB. Note that it will take a few seconds for the extension to load, so you should generally wait until the \"Db2 Extensions Loaded\" message is displayed in your notebook. \n",
    "1. Click the cell below\n",
    "2. Click **Run**. When the cell is finished running, In[*] will change to In[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5f185a4-558c-4b67-af88-c2a828445ace"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/IBM/db2-jupyter/master/db2.ipynb\n",
    "!wget -O db2.ipynb https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/db2.ipynb\n",
    "\n",
    "%run db2.ipynb\n",
    "print('db2.ipynb loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce970019-1b47-45b6-bf1f-3da2c4d5b016"
   },
   "source": [
    "#### Connecting to Data Virtualization\n",
    "\n",
    "Before any SQL commands can be issued, a connection needs to be made to the Data Virtualization system you will be using. \n",
    "\n",
    "The Db2 magic command tracks whether or not a connection has occured in the past and saves this information between notebooks and sessions. When you start up a notebook and issue a command, the program will reconnect to the database using your credentials from the last session. In the event that you have not connected before, the system will prompt you for all the information it needs to connect. This information includes:\n",
    "\n",
    "- Database name\n",
    "- Hostname\n",
    "- PORT \n",
    "- Userid\n",
    "- Password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a21f500-7602-462e-bcb8-9d82a6b47b1f"
   },
   "source": [
    "#### Connecting to Data Virtualization SQL Engine\n",
    "Run the cell below. Your DATAENGINEER username is automatically used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36496e94-8a6d-4073-8104-91c7ca22b623"
   },
   "outputs": [],
   "source": [
    "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\n",
    "database = 'bigsql'\n",
    "user = 'admin'\n",
    "password = 'password'\n",
    "host = '10.1.1.1'\n",
    "port = '32601'\n",
    "\n",
    "%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7b5bff22d4b4aa88830e13538f38bb1"
   },
   "source": [
    "Now that you are connected to the Data Virtualization engine you can query the virtualized tables using all the power in the Db2 SQL query engine. \n",
    "\n",
    "There are five tables that are available in each of the data sources. The examples below use a variety of data sources. For simplicity the schema of each data source represents where the data comes from. For example the NETEZZA.STOCK_SYMBOLS table is a virtual table that retrieves it data from a NETEZZA database. For your own projects you can choose any schema. There is no requirement to choose a schema that represents the source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e78fbd03adf4b058a9d6ff3c3e0ea52"
   },
   "source": [
    "The first table is a list of customer accounts with the current total number of stock trading transactions and the current account balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2625f169aee846e899917c44f0fcc3dc"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from DATASTAXSPARK.STOCK_SYMBOLS ORDER BY SYMBOL FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be25578215be470eabdb256ae6e56607"
   },
   "source": [
    "The same table is also available from a different source. In this case the STOCK_SYMBOLS table from a virtualized CSV file is available as CSV.STOCK_SYMBOLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecb42360447b4ec78c8928a01ab0d288"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from CSV.STOCK_SYMBOLS ORDER BY SYMBOL FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9de2c837e13a4b9a86680ebc8e62744e"
   },
   "source": [
    "To keep things simple, the virtualized data sources used in these examples have been encapsulated in views each using the TRADING schema. For example, the SQL below was used to map each virtualized table to a view. So you can create SQL to query the tables using the TRADING schema for each table. Later you can drop and recreate a view using a different source and keep the SQL you will run using the TRADING schema. \n",
    "\n",
    "    CREATE VIEW TRADING.CUSTOMERS AS SELECT * FROM CSV.CUSTOMER;\n",
    "    CREATE VIEW TRADING.STOCK_SYMBOLS AS SELECT * FROM DATASTAXSPARK.STOCK_SYMBOLS;\n",
    "    CREATE VIEW TRADING.STOCK_HISTORY_2 AS SELECT * FROM DB2WAREHOUSE.STOCK_HISTORY;\n",
    "    CREATE VIEW TRADING.STOCK_TRANSACTIONS AS SELECT * FROM EDBONCPD.STOCK_TRANSACTIONS;\n",
    "    CREATE VIEW TRADING.ACCOUNTS AS SELECT * FROM INFORMIX.ACCOUNTS;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7defc701dea64f358eeb03e2fde3837b"
   },
   "source": [
    "Now, let's explore some of the queries that we can run to gain insight from these tables. Have a look at the structure of each of the five tables. The represent trading information for a set of customers for a stock brokerage firm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95f9ed14414a426c82577e92be1e1d4b"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from TRADING.STOCK_SYMBOLS ORDER BY SYMBOL FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fadd292c1a840dab2c7ed8da509e86d"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from TRADING.CUSTOMER FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41cec17bd2e9413e9778f2f728731a62"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from TRADING.STOCK_SYMBOLS FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87a1b626d53243bda8aaee349371a8c3"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from TRADING.STOCK_HISTORY FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ab05c6bdad425ea3d851a706fa6474"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from TRADING.STOCK_TRANSACTIONS FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bb6bdd9-6856-4336-ba67-3f015ed53107"
   },
   "outputs": [],
   "source": [
    "%sql -a select * from TRADING.STOCK_SYMBOLS FETCH FIRST 10 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02f9df80-36f6-40a6-9798-5b5689a22839"
   },
   "source": [
    "### Building Insight from the Virtualized Tables\n",
    "Once you have tables virtualized you can use all the power of SQL and the Db2 SQL Engine to build complex and rich queries of your data no mater where it was originally located.\n",
    "#### Get Price of a Stock over the Year\n",
    "Set the Stock Symbol in the line below and run the cell. This information is folded together with data coming from two identical tables, one on Db2 database and on on and Informix database. Run the next two cells. Then pick a new stock symbol from the list above, enter it into the cell below and run both cells again.\n",
    "\n",
    "**Source Cloud Pak for Data - Db2 Warehouse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20c833e5-2f9b-4fd1-bc46-d9a4ed725b65"
   },
   "outputs": [],
   "source": [
    "stock = 'AXP'\n",
    "print('variable stock set to = ' + str(stock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca884c4d-39a1-4fde-8849-a90873a1a34f"
   },
   "outputs": [],
   "source": [
    "%%sql -pl\n",
    "SELECT WEEK(TX_DATE) AS WEEK, OPEN FROM DATASTAXSPARK.STOCK_HISTORY\n",
    "WHERE SYMBOL = :stock AND TX_DATE != '2017-12-01'\n",
    "ORDER BY WEEK(TX_DATE) ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f86739d9-5c33-43a2-a44b-53732da4c4e8"
   },
   "source": [
    "#### Trend of Three Stocks\n",
    "This chart shows three stock prices over the course of a year. It uses the same folded stock history information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d25ae779-2897-488c-8ef6-a48481a0ecd2"
   },
   "outputs": [],
   "source": [
    "stocks = ['INTC','MSFT','AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0184c0b8-ef98-4dc7-8347-8cb352b49a62"
   },
   "outputs": [],
   "source": [
    "%%sql -pl\n",
    "SELECT SYMBOL, WEEK(TX_DATE), OPEN FROM TRADING.STOCK_HISTORY\n",
    "WHERE SYMBOL IN (:stocks) AND TX_DATE != '2017-12-01'\n",
    "ORDER BY WEEK(TX_DATE) ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8cfe7b6-11e9-4c3e-a3c7-8302bf3119ce"
   },
   "source": [
    "#### 30 Day Moving Average of a Stock\n",
    "Enter the Stock Symbol below to see the 30 day moving average of a single stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c6f7c5c-1541-42d9-9180-9582cfb1e682"
   },
   "outputs": [],
   "source": [
    "stock = 'AAPL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efa8dcd2-275c-43d2-8b25-b72475408276"
   },
   "outputs": [],
   "source": [
    "sqlin = \\\n",
    "\"\"\"\n",
    "SELECT WEEK(TX_DATE) AS WEEK, OPEN, \n",
    "     AVG(OPEN) OVER (\n",
    "       ORDER BY TX_DATE\n",
    "     ROWS BETWEEN 15 PRECEDING AND 15 FOLLOWING) AS MOVING_AVG\n",
    "  FROM TRADING.STOCK_HISTORY\n",
    "     WHERE SYMBOL = :stock\n",
    "  ORDER BY WEEK(TX_DATE)\n",
    "\"\"\"\n",
    "df = %sql {sqlin}\n",
    "txdate= df['WEEK']\n",
    "sales = df['OPEN']\n",
    "avg = df['MOVING_AVG']\n",
    "\n",
    "plt.xlabel(\"Day\", fontsize=12);\n",
    "plt.ylabel(\"Opening Price\", fontsize=12);\n",
    "plt.suptitle(\"Opening Price and Moving Average of \" + stock, fontsize=20);\n",
    "plt.plot(txdate, sales, 'r');\n",
    "plt.plot(txdate, avg, 'b');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5c75afe-d4af-492b-804c-b5fe0318b3b7"
   },
   "source": [
    "#### Trading volume of INTC versus MSFT and AAPL in first week of November\n",
    "Enter three stock symbols below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a042c801-f88c-4d9f-8865-449208dd6d0a"
   },
   "outputs": [],
   "source": [
    "stocks = ['INTC','MSFT','AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cedb5123-f6b0-4bc3-ae7f-882a1ce7e0a3"
   },
   "outputs": [],
   "source": [
    "%%sql -pb\n",
    "SELECT SYMBOL, DAY(TX_DATE), VOLUME/1000000 FROM TRADING.STOCK_HISTORY\n",
    "WHERE SYMBOL IN (:stocks) AND WEEK(TX_DATE) =  45\n",
    "ORDER BY DAY(TX_DATE) ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ee32b6d-f7df-4823-8e70-2caf01ff0f94"
   },
   "source": [
    "#### Show Stocks that Represent at least 3% of the Total Purchases during Week 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fada678-59df-4a97-8789-e72d4aa76bf8"
   },
   "outputs": [],
   "source": [
    "%%sql -pie\n",
    "WITH WEEK45(SYMBOL, PURCHASES) AS (\n",
    "  SELECT SYMBOL, SUM(VOLUME * CLOSE) FROM TRADING.STOCK_HISTORY\n",
    "    WHERE WEEK(TX_DATE) =  45 AND SYMBOL <> 'DJIA'\n",
    "  GROUP BY SYMBOL\n",
    "),\n",
    "ALL45(TOTAL) AS (\n",
    "  SELECT SUM(PURCHASES) * .03 FROM WEEK45\n",
    ")\n",
    "SELECT SYMBOL, PURCHASES FROM WEEK45, ALL45\n",
    "WHERE PURCHASES > TOTAL\n",
    "ORDER BY SYMBOL, PURCHASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00e21b91-8d9e-4aa5-bb96-89210b657736"
   },
   "source": [
    "### Stock Transaction Table\n",
    "#### Show Transactions by Customer\n",
    "This next two examples uses data folded together from three different data sources representing three different trading organizations to create a combined of a single customer's stock trades. \n",
    "\n",
    "**Source EDB Postgres on Cloud Pak for Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75480f3f-9942-48b5-bc1e-a31081f5480c"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "SELECT * FROM TRADING.STOCK_TRANSACTIONS\n",
    " WHERE CUSTID = '107196'\n",
    " FETCH FIRST 10 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "021ef4b7-4494-40cb-9bb2-997820ade117"
   },
   "source": [
    "#### Bought/Sold Amounts of Top 5 stocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49558fcb-b19a-48f0-8b06-dfe73425d2b2"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "WITH BOUGHT(SYMBOL, AMOUNT) AS\n",
    "  (\n",
    "  SELECT SYMBOL, SUM(QUANTITY) FROM TRADING.STOCK_TRANSACTIONS\n",
    "  WHERE QUANTITY > 0\n",
    "  GROUP BY SYMBOL\n",
    "  ),\n",
    "SOLD(SYMBOL, AMOUNT) AS\n",
    "  (\n",
    "  SELECT SYMBOL, -SUM(QUANTITY) FROM TRADING.STOCK_TRANSACTIONS\n",
    "  WHERE QUANTITY < 0\n",
    "  GROUP BY SYMBOL\n",
    "  )\n",
    "SELECT B.SYMBOL, B.AMOUNT AS BOUGHT, S.AMOUNT AS SOLD\n",
    "FROM BOUGHT B, SOLD S\n",
    "WHERE B.SYMBOL = S.SYMBOL\n",
    "ORDER BY B.AMOUNT DESC\n",
    "FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c441f9e6-3ed3-4a3d-b262-4d2610724030"
   },
   "source": [
    "### Customer Accounts\n",
    "#### Show Top 5 Customer Balance\n",
    "These next two examples use data from an Informix database\n",
    "\n",
    "**Source - Informix on Premises**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0935077-2bf9-4fd7-b241-e70ece0be987"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "SELECT CUSTID, BALANCE FROM TRADING.ACCOUNTS\n",
    "ORDER BY BALANCE DESC\n",
    "FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44b57c79-45c5-408d-958b-1b1fc9a9600c"
   },
   "source": [
    "#### Show Bottom 5 Customer Balance\n",
    "**AWS - Db2, Azure - Postgres, Azure - Db2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da6426c9-9dc0-4f18-aed1-2e5938831e9a"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "SELECT CUSTID, BALANCE FROM TRADING.ACCOUNTS\n",
    "ORDER BY BALANCE ASC\n",
    "FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "045e094c-dc78-4550-b7da-622b2cb234bd"
   },
   "source": [
    "### Selecting Customer Information from MongoDB\n",
    "The MongoDB database (running on premises) has customer information in a document format. In order to materialize the document data as relational tables, a total of four virtual tables are generated by the Data Virtualization engine. The following query shows the tables that are generated for the Customer document collection and how to join them into a single relational table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de44d576-da63-4629-afce-2e07e3fe6cf6"
   },
   "outputs": [],
   "source": [
    "%sql -a select TABSCHEMA, TABNAME, COLCOUNT from syscat.tables where TABSCHEMA = 'MONGO' and TABNAME like 'CUSTOMER%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00100aa0-cc89-422e-9167-eae9a6574281"
   },
   "source": [
    "The tables are all connected through the CUSTOMERID field, which is based on the generated _id of the main CUSTOMER colllection. In order to reassemble these tables into a document, we must join them using this unique identifier. An example of the contents of the CUSTOMER_CONTACT table is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38e57e56-c5bd-4d59-bee2-0f9586fd8da9"
   },
   "outputs": [],
   "source": [
    "%sql -a SELECT * FROM MONGO.CUSTOMERS_CONTACT FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acc7f9b-cd8f-460e-8dd2-26de34904f14"
   },
   "source": [
    "A full document record is shown in the following SQL statement which joins all of the tables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6f5dabd-7202-44f3-9563-39eaef4aee79"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "SELECT C.CUSTOMERID AS CUSTID, \n",
    "       CI.FIRSTNAME, CI.LASTNAME, CI.BIRTHDATE,\n",
    "       CC.CITY, CC.ZIPCODE, CC.EMAIL, CC.PHONE, CC.STREET, CC.STATE,\n",
    "       CP.CARD_TYPE, CP.CARD_NO\n",
    "FROM MONGO.CUSTOMERS C, MONGO.CUSTOMERS_CONTACT CC, \n",
    "     MONGO.CUSTOMERS_IDENTITY CI, MONGO.CUSTOMERS_PAYMENT CP\n",
    "WHERE  CC.CUSTOMERS_ID = C.\"_ID\" AND\n",
    "       CI.CUSTOMERS_ID = C.\"_ID\" AND\n",
    "       CP.CUSTOMERS_ID = C.\"_ID\"\n",
    "FETCH FIRST 3 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2192437-a155-4514-aa66-2ec6e3821a0b"
   },
   "source": [
    "### Joining Virtualized Data\n",
    "In this final example we use data from four different data sources to answer a complex business question. \"What are the names of the customers in Ohio, who bought the most during the highest trading day of the year (based on the Dow Jones Industrial Index)?\" \n",
    "\n",
    "**Data Sources: Db2 Warehouse, Infomrix, MongoDB, Enterprise Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aa432f4-03da-4a55-9502-288c2e6adb85"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "WITH MAX_VOLUME(AMOUNT) AS (\n",
    "  SELECT MAX(VOLUME) FROM TRADING.STOCK_HISTORY\n",
    "    WHERE SYMBOL = 'DJIA'\n",
    "),\n",
    "HIGHDATE(TX_DATE) AS (\n",
    "  SELECT TX_DATE FROM TRADING.STOCK_HISTORY, MAX_VOLUME M\n",
    "    WHERE SYMBOL = 'DJIA' AND VOLUME = M.AMOUNT\n",
    "),\n",
    "CUSTOMERS_IN_OHIO(CUSTID, LASTNAME) AS (\n",
    "  SELECT C.CUSTOMERID, CI.LASTNAME\n",
    "    FROM  MONGO.CUSTOMERS C, \n",
    "          MONGO.CUSTOMERS_CONTACT CC,\n",
    "          MONGO.CUSTOMERS_IDENTITY CI\n",
    "    WHERE CC.CUSTOMERS_ID = C.\"_ID\" AND\n",
    "          CI.CUSTOMERS_ID = C.\"_ID\" AND\n",
    "          CC.STATE = 'OH'\n",
    "),\n",
    "TOTAL_BUY(CUSTID,TOTAL) AS (\n",
    "  SELECT C.CUSTID, SUM(SH.QUANTITY * SH.PRICE) \n",
    "    FROM CUSTOMERS_IN_OHIO C, TRADING.STOCK_TRANSACTIONS SH, HIGHDATE HD\n",
    "  WHERE SH.CUSTID = C.CUSTID AND\n",
    "        SH.TX_DATE = HD.TX_DATE AND \n",
    "        SH.QUANTITY > 0 \n",
    "  GROUP BY C.CUSTID\n",
    ")\n",
    "SELECT C.LASTNAME, T.TOTAL \n",
    "  FROM CUSTOMERS_IN_OHIO C, TOTAL_BUY T\n",
    "WHERE C.CUSTID = T.CUSTID\n",
    "ORDER BY TOTAL DESC\n",
    "FETCH FIRST 5 ROWS ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb83524c-020d-4945-9a73-00c6ed1e17c3"
   },
   "source": [
    "### Seeing where your Virtualized Data is coming from\n",
    "You may eventually work with a complex Data Virtualization schema with dozens or hundres of data sources. As an administrator or a Data Scientist you may need to understand where data is coming from. \n",
    "\n",
    "Fortunately, the Data Virtualization engine is based on Db2. It includes the same catalog of information as does a Db2 database with some additional features. If you want to work backwards and understand where each of your virtualized tables comes from. The list of virtualized tables is included in the **SYSCAT.NICKNAMES** catalog table. \n",
    "\n",
    "Run the following SQL to see all the virtual tables in your Data Virtualization system. We exclude the **DVSYS** schema since it contains system created virtual tables (nicknames), not user created virtual tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16738aff-a3a7-4372-8bc0-d269b77abaeb"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "SELECT TABSCHEMA, TABNAME\n",
    "  FROM SYSCAT.NICKNAMES\n",
    "    WHERE TABSCHEMA != 'DVSYS'\n",
    "    ORDER BY TABSCHEMA, TABNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1624ed1bf4b54d0593195b5fa89aa640"
   },
   "source": [
    "If you want to see exactly where a virtualized table comes from you can substitute the the schema and virtual table name in the following SQL procedure. For example the next cell retrieves the location of the NETEZZA.STOCK_SYMBOLS table. You can see the source schema name the source table name as well as the connection information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13141fa98bea43c39fa29a561f7d0410"
   },
   "outputs": [],
   "source": [
    "%%sql -a \n",
    "select * from table(dvsys.GET_VT_SOURCES('DATASTAXSPARK', 'STOCK_SYMBOLS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6451ed9851894f0f818b68cf5ea60487"
   },
   "source": [
    "You can also find the same information in the Cloud Pak for Data user interface. Look for the **Metadata** option in the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97f90eb64a48425f8f7a268948b9701f"
   },
   "source": [
    "To see the source of all your virtual tables we just need to join the query and the procedure call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e48a389dd01418594203c422f3adf75"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "SELECT N.TABSCHEMA AS TABSCHEMA, N.TABNAME AS TABNAME, S.SRCTABNAME AS SRCTABNAME, S.SRCSCHEMA AS SRCSCHEMA, S.SRCTYPE AS TYPE, S.DRIVER AS DRIVER, S.URL AS URL, S.USER AS USER, S.HOSTNAME AS HOSTNAME, S.PORT AS PORT, S.DBNAME AS DBNAME\n",
    "  FROM SYSCAT.NICKNAMES N, TABLE(\n",
    "  DVSYS.GET_VT_SOURCES(N.TABSCHEMA, N.TABNAME)) S\n",
    "  WHERE N.TABSCHEMA != 'DVSYS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68881735-b27d-47f7-9a86-cf5869decd5d"
   },
   "source": [
    "As part of this lab, a view had already been created in the DV engine that encapsulates the query above. It is the **ADMIN.REMOTETABLESOURCE** view. Run the following SQL to see how it works. Notice that you can use it just like any table and qualify the results with a where or order by clause. In this example we are listing all the virtual tables that come from a Netezza source server.\n",
    "\n",
    "Access has been granted to all users to this view. But you can choose to limit who can use it by granting or revoking permissions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e20ffc633a5445a7af8c853b736245f8"
   },
   "outputs": [],
   "source": [
    "%%sql -a \n",
    "SELECT TABSCHEMA, TABNAME \n",
    "    FROM ADMIN.REMOTETABLESOURCE \n",
    "    WHERE DRIVER = 'com.simba.spark.jdbc41.Driver'\n",
    "    ORDER BY 'TABSCHEMA', 'TABNAME'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1edbbf86065441f8ebba99d3d6b7802"
   },
   "source": [
    "You can also find the which tables a view is dependent on by querying the SYSCAT.TABDEP table. Run the example below. There is a line in the result set for each table that a view is dependent on. For example in the results below, the TRADING.ACCOUNTS view is dependent on on virtual table: INFORMIX.ACCOUNTS. The OHIO query is much more complex and is dependent on a number of views and tables. For example it uses the TRADING.STOCK_HISTORY view which in turn pulls data from the DB2WAREHOUSE.STOCK_HISTORY table. Because of this both dependencies are listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1154d34209c2438bb86e2d2ab81a55c4"
   },
   "outputs": [],
   "source": [
    "%%sql -a\n",
    "select tabschema,\n",
    "       tabname,\n",
    "       bschema as dependent_schema,\n",
    "       bname as dependent_name\n",
    "from syscat.tabdep\n",
    "where dtype = 'V'\n",
    "      and tabschema not like 'SYS%'\n",
    "      and tabschema = 'TRADING'\n",
    "order by tabschema, tabname, dependent_schema, dependent_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ff53cb6-edbb-4990-8a1f-d2fb9ec15525"
   },
   "source": [
    "### Check and Classify your Virtualized Data\n",
    "The STOCK data has already been added to the Watson Knowledge catalog.\n",
    "1. Select **Catalogs** and **All catalogs** from the main Cloud Pak for Data menu\n",
    "2. Click **Default Catalog**\n",
    "3. Enter **CUSTOMER** into the search bar\n",
    "4. Click the **DATASTAXSPARK.CUSTOMER** table\n",
    "5. Click the **Profile** tab. Scroll to the right to check the CARD_NO column. If profiling is complete you should see details on all the columns except for CARD_NO.  This includes the distribution of values in each column. Notice that the Credit Card Number column is unavailable because the data in the column is anonymized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c494208d-d8f7-416f-a4bc-36fdc860d53f"
   },
   "source": [
    "### Review the existing Data Protection Rules\n",
    "Now we can check the rules that are already in place that control how users can access our virtualized data.\n",
    "1. Select **Governace** and **Rules** from the main Cloud Pak for Data menu\n",
    "<img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/Rules.png\">\n",
    "2. Click **Mask Credit Card Numbers**\n",
    "<img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/MaskCreditCardNumbers.png\">\n",
    "3. Click **Edit**. While, you are not going to nake any changes to the rule, you can review the options.\n",
    "<img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/RuleEdit.png\">\n",
    "4. Click **Cancel**\n",
    "\n",
    "You can find out more about Data Protection Rules in the Cloud Pak for Data Knowledge Center: https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_current/wsj/governance/dmg_rules.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16dca2ed-6527-48ce-93a4-e110e9f0571a"
   },
   "source": [
    "### Preview Data in the Cloud Pak for Data Console\n",
    "Once you data has been classified and profiled protection is in place. Following the established protection rule, the credit card number will be redacted (replaced with Xs) everywhere it is accessible through Cloud Pak for Data. As you saw in the last step, it is reacted when you, and other users, interact with it through Watson Knowledge Catalog. \n",
    "\n",
    "It is also redacted if you access the data through the Data Virtualization console. Complete the following steps to see an example:\n",
    "\n",
    "First, switch to the LABUSER id to check that you can see the data you have just granted access for:\n",
    "9. Click the user icon at the very top right of the console\n",
    "10. Click **Log out**\n",
    "11. Sign in using the LABUSER id \n",
    "12. Click the three bar menu at the top left of the IBM Cloud Pak for Data console\n",
    "13. Select **Data Virtualization**\n",
    "\n",
    "Next, preview the data your just protected:\n",
    "1. Select **Data** and **Data virtualization** from the main Cloud Pak for Data menu\n",
    "2. Select **My virtualized data** from the Data virtualization menu\n",
    "3. Enter **CUSTOMER** in the **Find virtual objects** search\n",
    "4. Click the **DATASTAXSPARK_CUSTOMER** table\n",
    "5. Click the elipsis menu to the right of that row\n",
    "6. Select **Preview**. The CARD_NO column should be redacted with Xs.\n",
    "<img src=\"https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/media/MaskedVirtualTable.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "820e29c7-a747-46d3-a2b4-c3fc649f96fb"
   },
   "source": [
    "### Test Data Protection\n",
    "Importantly, the Credit Card information is protected when the data is access from an outside application. Let's reconnect to the Data Virtualization Engine as your DATAENGINEER user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33b38c927dce4c3b85eb4782ffcc9d21"
   },
   "outputs": [],
   "source": [
    "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\n",
    "database = 'bigsql'\n",
    "user = 'dataengineer1'\n",
    "password = 'tsdvlab'\n",
    "host = '10.1.1.1'\n",
    "port = '32601'\n",
    "\n",
    "%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8357dc119f2846729ba838754b37dd80"
   },
   "source": [
    "Now we can run a test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7d3d371-3ebb-469a-aa7b-6a21f5c8bd0e"
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM DATASTAXSPARK.CUSTOMER FETCH FIRST 10 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea8ff1c5a79f4f26892853df8e60b28f"
   },
   "source": [
    "You should see the CARD_NO column results only return Xs. Any application that accesses this data through the Data Virtualization engine will be redacted in the same way. Remember you are still connected through Python as the USER id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb02be21e22149a58f3044c09848ac5f"
   },
   "source": [
    "## Connect to Data Virtualization Using RESTful Endpoint Services\n",
    "The following is a brief example of how to use the Db2 11.5.4 RESTful Endpoint service to extend the capabilies of Cloud Pak for Data Virtualization. \n",
    "\n",
    "You can extend your Cloud Pak for Data system so that application programmers can create Representational State Transfer (REST) endpoints that can be used to interact with the Data Virtualization Service. \n",
    "\n",
    "Each endpoint is associated with a single SQL statement. Authenticated users of web, mobile, or cloud applications can use these REST endpoints from any REST HTTP client without having to install any Db2 drivers.\n",
    "\n",
    "The Db2 REST server accepts an HTTP request, processes the request body, and returns results in JavaScript Object Notation (JSON).\n",
    "\n",
    "The Db2 REST server is pre-installed and running on Docker on server7 (10.1.1.12) in the Demonstration cluster. As a programmer you can communicate with the service on port 50050. Your welcome note includes the external port you can use to interact with the Db2 RESTful Endpoint service directly.\n",
    "\n",
    "You can find more information about this service at: https://www.ibm.com/support/producthub/db2/docs/content/SSEPGG_11.5.0/com.ibm.db2.luw.admin.rest.doc/doc/c_rest.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c43e480cbe94f8983da4ea633f27c26"
   },
   "source": [
    "### Connecting to the RESTful Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cddedea55c64444297f8baef63cd9a42"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the Connection Header\n",
    "headers = {\n",
    "  \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the location of the RESTful Endpoint Service\n",
    "Db2RESTful = \"http://10.1.1.12:50050\"\n",
    "\n",
    "# Define the API call\n",
    "API_Auth = \"/v1/auth\"\n",
    "\n",
    "# Define the Data Virtualization Host on Cloud Pak for Data\n",
    "body = {\n",
    "  \"dbParms\": {\n",
    "    \"dbHost\": \"10.1.1.1\",\n",
    "    \"dbName\": \"bigsql\",\n",
    "    \"dbPort\": 32601,\n",
    "    \"isSSLConnection\": False,\n",
    "    \"username\": \"ADMIN\",\n",
    "    \"password\": \"password\"\n",
    "  },\n",
    "  \"expiryTime\": \"300m\"\n",
    "}\n",
    "\n",
    "# Try Connecting \n",
    "try:\n",
    "    response = requests.post(\"{}{}\".format(Db2RESTful,API_Auth), headers=headers, json=body)\n",
    "except Exception as e:\n",
    "    print(\"Unable to call RESTful service. Error={}\".format(repr(e)))\n",
    "print(response)\n",
    "if (response.status_code == 200):\n",
    "  token = response.json()[\"token\"]\n",
    "  print(\"Token: {}\".format(token))\n",
    "else: \n",
    "  print(response.json()[\"errors\"])\n",
    "\n",
    "# Using the token define a reusable header for subsequent calls\n",
    "headers = {\n",
    "  \"authorization\": f\"{token}\",\n",
    "  \"content-type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d498d3dc51ee405a9d207fbe2f8ca269"
   },
   "source": [
    "## Executing an SQL Statement\n",
    "Executing SQL requires a different service endpoint. In this case we will use \"/services/execsql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b4d5b50d18f444d8a5025275b069676"
   },
   "outputs": [],
   "source": [
    "API_execsql = \"/v1/services/execsql\"\n",
    "\n",
    "body = {\n",
    "  \"isQuery\": True,\n",
    "  \"sqlStatement\": \"SELECT * FROM DATASTAXSPARK.STOCK_TRANSACTIONS WHERE CUSTID = '107196'\",\n",
    "  \"sync\": True\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(\"{}{}\".format(Db2RESTful,API_execsql), headers=headers, json=body)\n",
    "except Exception as e:\n",
    "    print(\"Unable to call RESTful service. Error={}\".format(repr(e)))\n",
    "    \n",
    "print(response)\n",
    "\n",
    "display(pd.DataFrame(response.json()['resultSet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "178043bdc38643fab1ef364defcd5621"
   },
   "source": [
    "### Create a RESTful Service\n",
    "The most common way of interacting with the service is to fully encapsulate an SQL statement, including any parameters, in a unique RESTful service. This creates a secure separation between the database service and the RESTful programming service. It also allows you to create versions of the same service to make maintenance and evolution of programming models simple and predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f2bc7f84dd546f49481a41c928e9e70"
   },
   "outputs": [],
   "source": [
    "#Define service call\n",
    "API_makerest = \"/v1/services\"\n",
    "\n",
    "body = {\"isQuery\": True,\n",
    "       \"parameters\": [\n",
    "         {\n",
    "         \"datatype\": \"VARCHAR(6)\",\n",
    "         \"name\": \"@CUSTID\"\n",
    "         }\n",
    "       ],\n",
    "       \"schema\": \"STOCK\",\n",
    "       \"serviceDescription\": \"Get transactions given customer id\",\n",
    "       \"serviceName\": \"gettransactions\",\n",
    "       \"sqlStatement\": \"SELECT * FROM DATASTAXSPARK.STOCK_TRANSACTIONS WHERE CUSTID = @CUSTID\",\n",
    "       \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "# Try creating service\n",
    "try:\n",
    "    response = requests.post(\"{}{}\".format(Db2RESTful,API_makerest), headers=headers, json=body)\n",
    "except Exception as e:\n",
    "    print(\"Unable to call RESTful service. Error={}\".format(repr(e)))\n",
    "    \n",
    "if (response.status_code == 201):\n",
    "  print(\"Service Created\")\n",
    "else:\n",
    "  print(response.json()['errors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72af5d21bea84c12853261fb97f5cc0e"
   },
   "source": [
    "## Call the new RESTful Service\n",
    "Now you can call the RESTful service. In this case we will pass the stock symbol CAT. But like in the previous example you can try rerunning the service call with different stock symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a23961b17f0d42558c738b815cab7e88"
   },
   "outputs": [],
   "source": [
    "API_runrest = \"/v1/services/gettransactions/1.0\"\n",
    "\n",
    "body = {\n",
    "  \"parameters\": {\n",
    "    \"@CUSTID\": \"107196\"\n",
    "  },\n",
    "  \"sync\": True\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(\"{}{}\".format(Db2RESTful,API_runrest), headers=headers, json=body)\n",
    "except Exception as e:\n",
    "    print(\"Unable to call RESTful service. Error={}\".format(repr(e)))\n",
    "    \n",
    "print(response)\n",
    "\n",
    "display(pd.DataFrame(response.json()['resultSet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c682a6b294b142fe9bdec229ef0abeeb"
   },
   "source": [
    "## Delete a Service\n",
    "A single call is also available to delete a service. A response of 204 indicates success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daef89da58da4ac2b6c5b5da5cdd50f8"
   },
   "outputs": [],
   "source": [
    "API_deleteService = \"/v1/services\"\n",
    "Service = \"/gettransactions\"\n",
    "Version = \"/1.0\"\n",
    "\n",
    "try:\n",
    "    response = requests.delete(\"{}{}{}{}\".format(Db2RESTful,API_deleteService,Service,Version), headers=headers)\n",
    "except Exception as e:\n",
    "    print(\"Unable to call RESTful service. Error={}\".format(repr(e)))\n",
    "    \n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1090abff-db1f-4abc-8bd6-7d1023fcc370"
   },
   "source": [
    "#### Credits: IBM 2021, Peter Kohlmann [kohlmann@ca.ibm.com]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acf3c5a67d37402fbaa0cfeea9ead840"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
